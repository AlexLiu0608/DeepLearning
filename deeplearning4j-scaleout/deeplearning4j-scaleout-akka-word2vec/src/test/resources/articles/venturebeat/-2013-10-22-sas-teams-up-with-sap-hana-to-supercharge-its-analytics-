Updated Oct. 23 at 11:15 AM PT with a quote from SAP’s former chief scientist
The business analytics software from SAS is about to get a lot faster.
SAS will work with SAP to integrate its analytics software with SAP HANA, an in-memory computing platform, the two companies announced at SAP’s TechEd event in Las Vegas on Tuesday.
SAP and SAS will build and prioritize the bilateral technology throughout 2014, co-selling solutions as part of a pilot program that will focus on financial services, telecommunications, retail, consumer products, and manufacturing.
“It can be more efficient to move the model to the data than the data to the model,” said Henry Morris, the executive lead for IDC’s big data research, in a statement. “This relationship will significantly drive value to joint customers.”
Editor’s note: Our upcoming DataBeat conference , Dec. 4-Dec. 5 in Redwood City, will focus on the most compelling opportunities for businesses in the area of big data analytics and beyond. Register today!
But, at first glance, SAP’s former chief scientist doesn’t think the partnership seems very significant.
“I don’t think much of these kinds of marketing blurbs rapidly put together to coincide with an industry show,” said Ike Nassi, who is now founder and CEO of TidalScale . “Generally, what this means is that if a SAP sales rep calls on a customer that might need a SAS product — identified as such because he’s been given some sort of prep briefing materials from SAS — he picks up the phone and calls the appropriate SAS rep in his territory, and vice versa.”
HANA’s latest release will feature user experience design services and new cloud-based deployment options, SAP announced today. The platform, which became publicly available in 2011, now has more than 2,200 customers.
SAP foresees a billion-dollar future for HANA. In the company’s third quarter earnings call, SAP projected HANA revenues between $896 and $964 million for 2013, up from $540 million last year.
In-memory computing solutions are attractive for companies that generate massive amounts of data — we’re talking terabytes or petabytes — because they don’t need to pull data from servers’ hard drives. Instead, they store data in the computer’s main memory, which is faster and easier on the CPU. This speeds up processing by many orders of magnitude, allowing enterprises to process and analyze data sets that were previously inscrutable, including real-time data streams.
Related articles
