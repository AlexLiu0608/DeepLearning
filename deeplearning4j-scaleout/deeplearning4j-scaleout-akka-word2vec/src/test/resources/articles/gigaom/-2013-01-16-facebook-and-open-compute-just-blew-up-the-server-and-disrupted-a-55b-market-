photo: Stacey Higginbotham
Summary:
Facebook took aim at the hardware business back in April 2011 with the launch of the Open Compute open hardware program, and Wednesday it fired the killing blow at the $55 billion server business.
photo: Stacey Higginbotham
The launch of two new features into the Open Compute hardware specifications on Wednesday has managed to do what Facebook has been threatening to do since it began building its vanity-free hardware back in 2010. The company has blown up the server — reducing it to interchangeable components.
With this step it has disrupted the hardware business from the chips all the way up to the switches. It has also killed the server business, which IDC estimates will bring in $55 billion in revenue for 2012.
It’s something I said would happen the day Facebook launched the Open Compute project back in April 2011, and which we covered again at our Structure event last June when we talked to Frank Frankovsky, VP of hardware and design at Facebook. What Facebook and others have managed to do — in a relatively short amount of time by hardware standards — is create a platform for innovation in hardware and for the data center that will allow companies to scale to the needs of the internet, but also to do so in something closer to web-time. It will do this in a way that creates less waste, more efficient computing and more opportunity for innovation.
New features for a next-gen server
New Open Compute technologies on a rack.
So what is Facebook doing? Facebook has contributed a consistent slot design for motherboards that will allow customers to use chips from any vendor. Until this point, if someone wanted to use AMD chips as opposed to Intel chips, they’d have to build a slightly different version of the server. With what Frankovsky called both “the equalizer” and the “group hug” slot, an IT user can pop in boards containing silicon from anyone. So far, companies including AMD, Intel, Calxeda and Applied Micro are committing to building products that will support that design.
The other innovation that’s worth noting on the Open Compute standard is that Intel plans to announce a super-fast networking connection based on fiber optics that will allow data to travel between the chips in a rack. This 100 gigabit Ethernet photonic connector is something Intel plans to announce later this year, and Frankovsky can’t wait to get it into production in Facebook’s data centers.
I’ve written a lot about how we need to get faster, fiber-based interconnects inside the data center, and the efforts to do so . What’s significant here is not just that this design speeds up chip-to-chip communication. With the right hardware, it runs a rack into a server and makes the idea of a top-of-rack switch irrelevant –something that that Cisco, Juniper and Arista might worry about (although Arista’s Andy Bechtolsheim is helping present this technology at Open Compute, so I imagine he has a plan). It’s also worth noting that only in 2012 did Facebook transition to an all 10 gigabit Ethernet network in its data centers, but now it wants to speed up the interconnects as soon as it can.
Openness makes the new servers flexible
So what has happened here is significant. Open Compute has managed to give customers — from financial services firms to web properties — a platform on which to build custom and modular servers.
A good example of this might be building a house. There are plenty of ways of doing it, from hiring an architect to choosing a plan from a selection offered to you by a developer. That was the former server world — you either went all custom or chose from “plans” provided by Dell and others. But those chosen plans might come with four bathrooms and you might only want three. If you buy 10,000 unneeeded bathrooms, that’s a lot of added cost that brings you no value.
With Open Compute, Facebook showed people how to build houses out of shippping containers. As more and more elements like Group Hug or standard interconnects are added, it’s like you can pick your favorite brand of shipping container and pop it on. Even better, if you want a new bathroom, you can swap it out without ever affecting your bedroom. And for many companies the costs of building and operating these new servers will be less than the mass-produced boxes designed by others.
This is great for web-based businesses and anyone that relies on IT. It cuts out waste and it turns what was once a monolithic asset into something that can be upgraded with relative ease and at a lower cost. So when Frankovsky is talking about renting CPUs from Intel, for example, the Group Hug design makes that possible. He can rent CPUs for a workload, and then use those servers for something else without chucking them. He can swap out chip architectures, networking cards and drives at will to put the right mix of storage, network, and compute together for his jobs as they grow and shrink.
These servers can’t be provisioned as fast, but what Open Compute has done is create a server architecture that can scale and change as much as the cloud itself does. And it did so without the server guys.
Related research
Subscriber Content
?
Subscriber content comes from Gigaom Research, bridging the gap between breaking news and long-tail research. Visit any of our reports to learn more and subscribe.
