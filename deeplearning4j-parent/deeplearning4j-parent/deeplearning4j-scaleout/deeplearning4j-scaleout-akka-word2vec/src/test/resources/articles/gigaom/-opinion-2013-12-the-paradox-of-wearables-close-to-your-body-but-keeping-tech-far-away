<img class="size-full wp-image-167661" title="MG_0179edit-660x440" src="http://www.wired.com/opinion/wp-content/uploads/2013/12/MG_0179edit-660x440.jpg" alt="" width="660" height="440" />
Photo: Ariel Zambelich/WIRED
One of the rallying cries for Google Glass is to make technology that’s there when you need it, gone when you don’t. It is intended to help people get on with their lives, without focusing on the technology. Wearable computers , in general, play supporting roles in what the user is doing — as opposed to the computer use being the primary focus itself.
Vannevar Bush, one of the earliest computing pioneers, understood this idea and described wearable computers and cameras in the 1945 LIFE magazine version of his famous essay “As We May Think.” Unfortunately, only early mainframes existed then, and mainframes require that we bring ourselves to the computer. Slowly, though, the technology came to us: from computers on our desktops to laptops to mobile phones.
Bringing technology and computing closer to the body can actually allow technology to get further out of the way.
While these technologies lowered the barrier to communicating and accessing information, they created other barriers. We are now often greeted in meetings with a literal barrier of screens that get in the way of face-to-face communication.
Can we instead make devices that encourage in-person, face-to-face communication — while still delivering the data people need, only when they need it? I’ve been making and using wearable computers in my daily life since 1993 and have been a technical lead and manager on Google Glass since its first year in 2010. It may seem like a paradox, but I argue that bringing technology and computing closer to the body can actually improve communication and attention — allowing technology to get further out of the way.
All wearable computing designs make trade-offs between battery life and weight, size and fashion, simplicity of use and type of task supported. Glass is different both in its significantly different engineering choices and its interactions that exploit wearable computing’s main advantages (as opposed to replicating the interfaces of the PC or smartphone).
Wearable computers reduce the time between a user’s intentions to do a task and her first action to perform it; this reduction in time between intention and action is one of the main advantages. Wearable computers can also be very social devices in the interactions they enable and the contexts in which they are used. (For example, Glass is designed to allow easy sharing of the user’s experiences through 10-second video clips, photographs, and hangouts, and it also provides explicit social cues to conversational partners as to when it is being used). Finally, wearable computers can be designed to take a secondary, supporting role in the user’s life as opposed to being the main focus of the user’s attention.
Designing for Social Transparency and Flow
When I started the MIT Wearable Computing Project in 1995, the group of us who wore computers on a daily basis soon discovered that our conversational partners did not understand the computers’ purpose. They mistakenly thought that we were constantly “living in VR.”
In response, making the interface more socially transparent became a goal. I’ll use Glass, which has been my most recent experience, to illustrate. Take the display. We decided to mount the display high so that there would be no barrier to eye contact. Even if the user bends her head down and looks through the display, her eyes can still be seen. When the display is on, both the user and her conversational partners can see it. This design makes clear when the device is in use.
<img style="width: 60px; height: 60px; float: left;" src="http://www.wired.com/opinion/wp-content/uploads/2013/12/thadstarner_wiredopinion.jpg" alt="" />
Thad Starner
Wearable computing pioneer Thad Starner is a Professor in the School of Interactive Computing at the Georgia Institute of Technology, and Technical Lead on Google Glass, which was named a Time Magazine Invention of the Year. He was perhaps the first to integrate a head-up display-based wearable computer into his everyday life as an intelligent agent, and is a founder of the annual ACM/IEEE International Symposium on Wearable Computers. Starner coined the term “augmented reality” in 1990 to describe the types of interfaces he envisioned at the time; has authored over 150 peer-reviewed scientific publications; and is an inventor on over 80 U.S. patents.
<img style="padding: 10px 0 0 0;" src="http://www.wired.com/opinion/wp-content/uploads//2012/05/op-bug-bg-bottom.gif" alt="" />
Going further, using Glass involves social actions. When I am asked a question, I might reply “I don’t know, let me look that up,” and I physically look up at Glass’s display as I trigger a Google search. Similarly, “OK Glass, take a picture” notifies my conversational group that a picture is being taken. Alternatively, the camera can be triggered by a button press at the corner of the frames. The gesture, which includes looking at my subject, maneuvering my head to the best angle, and clicking the “shutter,” is familiar. It resembles the actions I would perform on a normal pocket camera. By making the use of Glass apparent, it becomes a shared social experience.
To the user, the position of the display is similar to that of a rearview mirror in a car: up and out of the way from the user’s normal eye-line. Interactions are designed to be brief. Like a rearview mirror, a person can glance up briefly at the display to get the information she needs before quickly returning to look forward. The 640×360 pixel display appears focused some distance away from the body. That way, a person can see the display without changing the physical focus of her eyes from what she is interacting with in the world.
This design focus on keeping the user’s head up is key to keeping them “in the flow” and aware of the world around them, whether it be in conversation or while walking down the street. Parents capturing their child’s recital keep their heads up using Glass instead of living the experience looking down through their camcorder’s viewfinder. As much as possible, Glass takes the technology out of the way so that the user can focus on what matters.
Designing for Conversation
Being able to access web search with a glance is surprisingly powerful during face-to-face conversation. Turning our heads to the physical barrier of a laptop or smartphone display means we are no longer focusing on what our colleagues or friends or family are saying. By the time we return with whatever information we are seeking, the conversation has moved on.
That last point is key. The biggest barrier to using a smartphone during conversation is the amount of time required to use it. My studies of scheduling appointments during conversation revealed that people often balk at using their mobile devices. Retrieving a device, unlocking it, and then navigating the interface to the appropriate place to start the search removes the user from the conversation for over 20 seconds.
Let me pause on that for a moment. In fact, count out twenty seconds before reading the next sentence. 1…….. 2…….. 3…….. 4…….. 5…….. 6…….. 7…….. 8…….. 9…….. 10…….. 11…….. 12…….. 13…….. 14…….. 15…….. 16…….. 17…….. 18…….. 19…….. 20. Done? Did you actually wait the 20 seconds? Probably not, because it soon seems awkward and disrupts the flow of your reading. Imagine that delay, plus more to actually use the application, during a conversation.
The worse part is that people stall and put in filler speech to cover up the awkward waiting or silence. For example, in response to a colleague asking “Can we meet at 4 p.m.?” the script I’ve observed is:
Person reaches in pocket or purse for his or her phone.
Says, “Hold on, let me check my schedule…”
Guesses while finding information: “I know I’m busy today…”
Retrieves device: “What time was that again?”
Confirms information and talks aloud: “4 p.m., right? … Let me scroll down…”
Answers question: “Yes, I can make it.”
Meanwhile, the person on the other end is stuck filling the delay with small talk, empty wait time, or going down their own rabbit-hole on their device, escalating the interruptive nature of the interaction.
The goal of technology should be to assist with the flow of human interaction, not misdirect the user’s attention to itself unnecessarily.
The design focus on keeping the user’s head up is key to keeping them ‘in the flow’ and aware of the world around them.
Reducing the Time Between Intention and Action
Wearables empower the user by reducing the time between their intention to do a task and their ability to perform that task. Glass wearers can perform useful tasks in two seconds: checking the time, taking a picture, checking the next appointment, reading the first line of an email to see if it’s important, doing a quick search, checking the weather, glancing at a text, starting a video.
Such microinteractions are the social equivalent of checking time on a wristwatch: noticeable, yet fast enough to be minimally disruptive and not really affect the conversation.
Designing for such microinteractions is particularly powerful for people whose jobs (knowledge workers, for instance) or lifestyles (single parents) require constant monitoring of their communications. Microinteractions are another way to keep people “in the flow” of what they are doing.
The conflicted social etiquette of being in a meeting with one’s cellphone — something many working parents experience — can be addressed by wearables like Glass. With laptops and smartphones, the desire is to monitor email and SMS but, in reality, the device’s interface draws the user into the device.
To address this reality, we intentionally limited the interaction on Glass to be quick and shallow through the design of the hardware and the software. For anything that requires more than a few seconds of interaction, the user moves to her laptop or mobile phone. This dichotomy requires the user to decide actively to engage the technology for a deeper interaction where their attention is dedicated to the device.
Glass is also designed for “ignorability”. By default, the display is off. With notifications, a small auditory “bing” indicates an incoming text or important email (in Gmail, a “starred” message as determined by the user’s filters). Users can ignore it or tilt their head up to display the notification. A nod then dismisses the message. The entire interaction is a shift in focus of two seconds instead of more than 20 just to begin.
Enabling Technology
Reducing the time between intention and action isn’t just a nice-to-have, it’s a must have. Yet it’s especially significant for those who have low vision or are blind and are confronted with mundane tasks that the sighted take for granted, like finding matching socks or determining if a can of food contains gluten or nuts.
Designing for microinteractions is particularly powerful for people whose jobs or lifestyles (single parents) require constant monitoring of their communications.
There are six million adults who are blind in the United States. To help address their needs, Google Glass Explorer Professor Jeff Bigham created “ VizWiz ,” an app that anonymously outsources answers to such questions to remote workers, and his collaborators Brandyn White and Andrew Miller brought the idea to Glass using Wearscript . The idea is to reduce the time and social barriers to getting help with everyday questions.
“In the real world,” Dr. Bigham observes, “users aren’t asking their questions in isolation. They’re trying to get something done — making dinner, taking a walk, caring for a loved one. Pulling out a phone to take a picture temporarily removes them from these activities.” For him, Glass is a way to “get people the information they need without disrupting them from what they actually care about.”
Meanwhile, my Georgia Tech group just released a prototype Android app and Glassware called “Captioning on Glass” for people who are hard-of-hearing. During a face-to-face conversation, one person speaks into a mobile phone that transcribes what she is saying and transmits the text to Glass’s head-up display for the person who is hard-of-hearing to read.
Many times a person who is hard-of-hearing can understand what their conversational partner is saying by the audio they do hear, the context, the facial gestures, and looking at the lips; the transcription fills in details when necessary. Thus, by having a head-up display, the wearer can stay “in the flow” of the conversation, attending the other person’s face to get as much information as possible while speeding the natural conversation.
From Microinteractions to Microlearning
In general, we can switch our focus of attention in split seconds, and we do such “multiplexing” all the time — while walking, eating, and conversing. In fact, our eyes start becoming uncomfortable if we try to hold them steady for more than 0.6 seconds. The problem occurs when two tasks overlap (especially language-based tasks) that require too much attention for too long, forcing an attempt at multitasking. Keeping interactions brief allows the user to remain in control.
Such microinteractions can also lead to microlearning.
In another example that applies to people who are disabled, my team at Georgia Tech is trying to help hearing parents of deaf children learn American Sign Language. Over 90% of deaf children are born to hearing parents, and parents have difficulty learning enough sign language to teach their children.
We do ‘multiplexing’ all the time. The problem occurs when two tasks overlap, forcing an attempt at multitasking.
Sometimes a deaf child’s first exposure to language is in grade school. Even when a parent manages to learn the basics of sign, learning new vocabulary is difficult. SMARTSign provides micro sign language lessons designed to teach the vocabulary parents need to learn to communicate with their infant. Dr. Kim Xu, who has studied the significant benefits of delivering sign language lessons on a mobile phone, has already ported  SMARTSign to Glass. Throughout the day, the user gets notifications of available micro lessons. When users have a spare minute they can find a video of a sign in their timeline on Glass. After the short video plays, the user is asked to select the equivalent concept in English from a multiple-choice test. Depending on how the user performs, that sign will be replayed more or less often in the future rotation of videos, until the user demonstrates familiarity with the sign. If they have more time, users can ask for more sign videos.
I recently learned 10 new signs while waiting in line to board an airplane. By using Glass to lower the barrier for parents learning sign, we hope to help deaf children acquire language skills by having their language more accessible to them in their homes.
* * *
These are only a few of the examples where Glass — and I’m sure other wearables — could lower barriers for people in their daily lives.
Each time computing has become more mobile, faster to access, and more interactive, there’s been a revolution in new services and ecosystems. However, this time I hope that by bringing the technology closer to us, it will, in effect, get more out of the way. That’s the true promise of wearables … and it may finally be here for everyone.
We Recommend
