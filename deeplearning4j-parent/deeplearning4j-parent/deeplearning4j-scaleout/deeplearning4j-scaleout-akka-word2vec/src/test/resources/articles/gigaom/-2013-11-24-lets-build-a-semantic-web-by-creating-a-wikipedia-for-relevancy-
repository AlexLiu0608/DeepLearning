[GENDER MALE]
[COMMERCIAL CRUSTACEANS]
This is like an ad server’s dream! Whoever tweeted this needs to be pummeled and retargeted with Red Lobster ads for months. I actually have set up sites with this sentence and tasked many other IAB-focused ad systems to recognize it — and it’s all Red Lobster all the time. I’ve enjoyed many half-off cheese biscuits in the last 12 months (R&D sometimes bears not only fruit but also cheesy biscuits).
But I wasn’t talking about canoeing or lobsters. When I’m not working I surf and, unfortunately, occasionally I do get sunburned — sometimes to the point of being told I look like a lobster. That’s what I was conveying in my tweet. Why is it so easy for us to understand but so hard for a machine to understand?
But maybe this is just a funny edge case. You can confuse any computer system if you try hard enough, right?
Unfortunately, this isn’t an edge case. Lexicons used to be considered different languages or different colloquial terms specific to particular industries before Twitter. This is no longer true: 140 characters has not just changed people’s tweets, it has changed how people talk on the web. More and more information is being communicated in smaller and smaller amounts of language, and this trend is only going to continue. #exponential
So why is there not a semantic web? Why can’t we solve this yet? Why can’t computers understand that “I’m a lobster” means you are sunburned and not that you want cheesy bread?
Not just connections, but connections that matter
I believe the reason that there are not hundreds of companies exploiting machine learning techniques to generate a truly semantic web is the lack of weighted edges in publicly available ontologies. “Lobster” and “sunscreen” are seven hops away from each other in DBpedia — way too many to draw any correlation between the two. (Any article in Wikipedia can be connected to any other article within about 14 hops, and that’s the extreme. Meanwhile, completely unrelated concepts are often just a few hops from each other.) But by analyzing massive amounts of both written and spoken English text from articles, books, Twitter and television, it is possible for a machine to automatically draw a correlation and create a weighted edge that effectively short circuits the sevens hops otherwise necessary.
Many organizations are dumping massive amounts of facts without weights into our repositories of total human knowledge because they are naively attempting to categorize everything without realizing that the repositories of human knowledge need to mimic how humans use knowledge.
For example: As of today, Kobe Bryant is categorized under 28 categories in Wikipedia, each of them with the same level of attachment (one hop in a breadth- or depth-first traversal).
But when you are at a coffee shop and overhear the person next to you mention Kobe Bryant, what are you able to infer they are talking about? “Basketball” or “American Roman Catholics”? How can the human brain infer that so quickly yet machines get so confused? It is not due to lack of technical processing power, Moore’s law slowing down or the thickness of our silicon wafers — it’s because of the data.
This is a small example of what  someone who works with graph theory would come up with if he or she were to run a standard few-hop depth first traversal from Kobe Bryant on Wikipedia and attempt to coalesce around a common category:
So when someone tweets about Kobe Bryant, are they talking about people born in 1970,Pennsylvania, Food & Drink, or Canadian Inventions? This is a common example of how confused a machine can become when the distance of unweighted edges between nodes is used as a scoring mechanism for relevancy.
But what happens if we weight our edges? The same Wikipedia nodes with path costs can be run through a traversal algorithm that calculates those costs and we get the following:
Our machine is starting to think like a human.
Algorithms and processors aren’t enough
Weighted path traversals are not new. Dijkstra’s algorithm was invented in 1956 (the answer has been around for a long time), but the processing power and memory necessary to leverage a traversal algorithm like Dijkstra’s — and score path costs and not just distance between nodes across massive data sets — has only in the last few years become available to the average startup. That’s a huge win for all of us, but the data and ontologies to actually do it are still not publicly available.
I propose as an industry we begin to focus more on relevancy and less on factual accuracy. The above flawed traversal is actually 100 percent factually accurate. Kobe Bryant was born in 1979, he is (or was) sponsored by Gatorade, Gatorade is a drink and basketball was invented in Canada. But even now that you know all those facts, when you hear someone talk about Kobe Bryant tomorrow you will still know they are talking about basketball.
The only way we will actually get to a truly semantic web is when machines are able to think (or, more accurately, perform) as we do. The processing power, technology and algorithms to do that exist today. Unfortunately, said power is unleashed on inherently flawed datasets, and that is why we still see Red Lobster ads on sunscreen pages. We need to become much less focused on adding facts to Freebase, DBpedia and the other publically available ontologies, and much more focused on weighting the edges between the facts that we are adding.
That is how we create a truly semantic web. The answer lies in the data, but not in the data available on a web page or in a set of thousands of web pages available to be recommended by a particular algorithm. Informational retrieval and categorization techniques such as LSI, PLSI,  and LDA are only aware of the context of the information in the datasets fed to them. These base algorithms (LDA, especially) are incredibly useful, but without the context of a global human knowledge base, you cannot build an interest graph, and you cannot build a semantic web.  
Ontologies become absolutely necessary as we attempt to solve this problem. If you feed into any of the above algorithms 10 articles a particular person read about snowboarding, they will successfully recommend other snowboarding articles, but are unaware that snowboarding and surfing are two sports that go hand in hand. People who enjoy one usually enjoy the other. An ontology with weighted edges is necessary to make that relevant yet tangential connection, which is a crucial step to avoid the dreaded “filter bubble.”
Benedetto (center) at Structure: Data 2012.
A Wikipedia for weighted edges
So to all of my semantic colleagues out there, maybe we should shift our thinking and begin to use a different yardstick to measure the quality of our knowledge repositories. For 99 percent of all use cases we have enough nodes. We have successfully deposited the majority of places, events, people, thoughts, and most other tangible and intangible things in the world into our data stores — and a good portion of the population has access to all of it from the smartphone in their pocket. That is an incredible feat. But it’s only half of the equation.  We still have yet to map the data into a format that mimics how the human mind thinks.
The way to do this is to begin weighting the edges that interconnect the nodes and facts that we are adding every day. It requires us to raise the bar from factually accurate to actually relevant. Kobe Byrant -> Philadelphia is factually accurate, but Kobe Bryant -> Basketball is actually relevant. Today’s ontologies make no distinction between those two facts, and without that distinction a machine will never be able to create the semantic web we have been working towards for almost a decade.
Every fact in Wikipedia was added by a human. Weighting all of the edges between those facts sounds like a monumental task. But crowdsourcing the creation of a central repository of all human knowledge sounded impossible a little over a decade ago, and we’ve done a very good job of that.
It wasn’t too long ago that running an elastic cloud infrastructure was something that was available to only the largest internet companies in the world. Amazon changed that. Now, one smart engineer can turn an idea into a company for $50 a month. But there is still a large divide between one smart engineer and companies that can use machines to perform web-scale semantic analysis of content.
The relevancy-defining, edge-weighting algorithms of Google’s Knowledge Graph, Facebook’s Open Graph and Gravity’s Interest Ontology are closely guarded company secrets. Imagine if that data was available to everyone — it would be as disruptive as Amazon Web Services. The internet would be a better place.
At Gravity, we have combined many publicly available ontologies with our own internally generated facts and weights to create a large  interest-based undirected graph that leverages many forms of edge weighting to solve the above problem. For many years, we built and protected this as a company secret. In the last year we have realized that our mission — building a web-scale personalization platform — takes a lot more then an ontology with weighted edges. It’s an iceberg problem that looks simple when you are designing collaborative filtering for an app or yield-optimizing by user for your site, but our mission is a platform for the entire web.
A relevancy-based ontology with weighted edges is absolutely necessary, but it is just the beginning. That’s we are also formalizing a plan to develop an open, centralized place to allow human and machine curation of ontology edge weights for the community. We plan on contributing a significant amount of our data to get the project started. More on that to come.
Until then, as a community, I believe we should begin to focus more on relevancy and relationships, and less on the continued addition of facts to our publicly available semantic resources.
Jim Benedetto is co-founder and CTO of Gravity.
Related research
Subscriber Content
?
Subscriber content comes from Gigaom Research, bridging the gap between breaking news and long-tail research. Visit any of our reports to learn more and subscribe.
